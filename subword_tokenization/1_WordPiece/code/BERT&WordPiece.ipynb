{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c67a6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf95c8f",
   "metadata": {},
   "source": [
    "## WordPieceTokenizer Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc4e420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5087c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vocabulary\n",
    "# you could skip this part if you already have a vocab lookup table or you want to use prepared vocab\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4885e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params = dict(lower_case=True)\n",
    "\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 1000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a91fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_usa.txt') as f:\n",
    "    str_wiki_usa = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d5c8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list_usa = str_wiki_usa.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01022ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Page', 'semi-protected', 'United', 'States', 'From', 'Wikipedia,', 'the', 'free', 'encyclopedia', 'Jump']\n"
     ]
    }
   ],
   "source": [
    "print(str_list_usa[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "383530fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 16:53:34.751568: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tmp = tf.data.Dataset.from_tensor_slices(str_list_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fd00fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int64, numpy=0>, <tf.Tensor: shape=(), dtype=string, numpy=b'Page'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=1>, <tf.Tensor: shape=(), dtype=string, numpy=b'semi-protected'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=2>, <tf.Tensor: shape=(), dtype=string, numpy=b'United'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=3>, <tf.Tensor: shape=(), dtype=string, numpy=b'States'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=4>, <tf.Tensor: shape=(), dtype=string, numpy=b'From'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=5>, <tf.Tensor: shape=(), dtype=string, numpy=b'Wikipedia,'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=6>, <tf.Tensor: shape=(), dtype=string, numpy=b'the'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=7>, <tf.Tensor: shape=(), dtype=string, numpy=b'free'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=8>, <tf.Tensor: shape=(), dtype=string, numpy=b'encyclopedia'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=9>, <tf.Tensor: shape=(), dtype=string, numpy=b'Jump'>)\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=10>, <tf.Tensor: shape=(), dtype=string, numpy=b'to'>)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for item in tmp.enumerate():\n",
    "    print(item)\n",
    "    if counter >= 10:\n",
    "        break\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ce8837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.73 s, sys: 24.9 ms, total: 9.76 s\n",
      "Wall time: 9.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#     train_pt.batch(1000).prefetch(2),\n",
    "#     **bert_vocab_args\n",
    "# )\n",
    "\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    tmp.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cba3a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '#', '$', '%', '&']\n",
      "['##s', 'in', 'states', 'united', 'retrieved', 'to', '##ing', 'american', '##e', '##ed']\n",
      "[]\n",
      "['##ع', '##ل', '##ي', '##–', '##—', '##•', '##′', '##−', '##中', '##文']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:10])\n",
    "print(en_vocab[100:110])\n",
    "print(en_vocab[1000:1010])\n",
    "print(en_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8eacb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "679c7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('en_vocab.txt', en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b02e4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build wordpiece tokenizer\n",
    "\n",
    "en_wordpiece_tokenzier = text.WordpieceTokenizer(\n",
    "    vocab_lookup_table=\"en_vocab.txt\", \n",
    "    suffix_indicator='##', \n",
    "    max_bytes_per_word=100,\n",
    "    max_chars_per_token=None, \n",
    "    token_out_type=tf.string,\n",
    "    unknown_token='[UNK]', \n",
    "    split_unknown_characters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b72261c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'h', b'##ell', b'##o'], [b'world', b'##,'], [b'm', b'##y'], [b'n', b'##ame'], [b'is'], [b'f', b'##ra', b'##n', b'##k']]>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_wordpiece_tokenzier.tokenize(\"hello world, my name is frank\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c96226df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build wordpiece tokenizer\n",
    "\n",
    "en_wordpiece_tokenzier = text.WordpieceTokenizer(\n",
    "    vocab_lookup_table=\"en_vocab.txt\", \n",
    "    suffix_indicator='##', \n",
    "    max_bytes_per_word=100,\n",
    "    max_chars_per_token=None, \n",
    "    token_out_type=tf.int32,\n",
    "    unknown_token='[UNK]', \n",
    "    split_unknown_characters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd66db0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[43, 640, 156], [122, 926], [320], [116], [36], [105, 552, 121, 174, 364, 127, 926]]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_wordpiece_tokenzier.tokenize(\"hello world, this is a tokenizer,\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "398ffd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 640, 156, 122, 926, 320, 116, 36, 105, 552, 121, 174, 364, 127, 926]\n"
     ]
    }
   ],
   "source": [
    "combined = []\n",
    "for item in [[43, 640, 156], [122, 926], [320], [116], [36], [105, 552, 121, 174, 364, 127, 926]]:\n",
    "    combined.extend(item)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d43c1212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'hello', b'world,', b'this', b'is', b'a', b'tokenizer,']]>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_wordpiece_tokenzier.detokenize([combined])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0113a9",
   "metadata": {},
   "source": [
    "## BertTokenizer Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73198f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert Tokenizer 可以使用相同的vocab但是tokenization不再需要句子pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f997872",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_bert_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "513746c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[43, 640, 156], [122], [15], [320], [116], [36], [105, 552, 121, 174, 364, 127], [15]]]>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_bert_tokenizer.tokenize(\"hello world, this is a tokenizer,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e07dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 640, 156, 122, 15, 320, 116, 36, 105, 552, 121, 174, 364, 127, 15]\n"
     ]
    }
   ],
   "source": [
    "combined = []\n",
    "for item in [[43, 640, 156], [122], [15], [320], [116], [36], [105, 552, 121, 174, 364, 127], [15]]:\n",
    "    combined.extend(item)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7dd670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'hello', b'world', b',', b'this', b'is', b'a', b'tokenizer', b',']]>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_bert_tokenizer.detokenize([combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88871f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
