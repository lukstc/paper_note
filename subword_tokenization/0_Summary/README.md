# Tokenization - Vocab



## WordPicec_local

for reference and learning.
Code Source: [Google-research/bert](https://github.com/google-research/bert)

## vocab_prepare

subword unit tokenization process is highly deplend on vocabulary/dictionary
vocabulary/dictionary preparation is seperate topic from tokenization itself.
Comparing to tokenization and NLP model traning, vocabulary/dictionary preparation is kind of borning and unnecessary (we could use the work doen by the others). But by learning the details of this process, we could have a better understanding of how does NLP model work and improve this data preparation, training, and predicting process.

## Pending

- [ ] SentencePiece: 
  - [ ] foo,bar
  - [ ] Start, end
- [ ] WordPiece
  - [ ] Vocab 准备
- [ ] BERT & Transformer实践
- [ ] BPE逻辑和算法实践