{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c3d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考Bert WordPiece的逻辑，local实现tokenization功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffa35d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d45a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"\"\"WordPiece_local/bert-large-uncased-vocab.txt\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5c8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a91636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path,\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        # print(line.strip())\n",
    "        vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426b59cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interpreted'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[10009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d64505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfde107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c09ce75",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1c9b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "temp = collections.OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ff7ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"AAA\"]=2\n",
    "temp[\"BBB\"]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2335bbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9b52124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('AAA', 2), ('BBB', 1)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a903de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8db13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3af18be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 11:25:46.845022: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Sponge bob Squarepants is an Avenger'>,\n",
       " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Barack Obama is the President.'>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = {\n",
    "    \"text_a\": [\n",
    "      b\"Sponge bob Squarepants is an Avenger\",\n",
    "      b\"Marvel Avengers\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     b\"Barack Obama is the President.\",\n",
    "     b\"President is the highest office\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d412e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "\n",
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 8\n",
    "_MAX_PREDICTIONS_PER_BATCH = 5\n",
    " \n",
    "_VOCAB_SIZE = len(_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fb9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "      keys=_VOCAB,\n",
    "      key_dtype=tf.string,\n",
    "      values=tf.range(\n",
    "          tf.size(_VOCAB, out_type=tf.int64), dtype=tf.int64),\n",
    "      value_dtype=tf.int64),\n",
    "      num_oov_buckets=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b475a608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[b'Sp', b'##onge'], [b'bob'], [b'Sq', b'##uare', b'##pants'], [b'is'], [b'an'], [b'A', b'##ven', b'##ger']], [[b'Mar', b'##vel'], [b'A', b'##ven', b'##gers']]]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.string)\n",
    "bert_tokenizer.tokenize(examples[\"text_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503b21d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[22, 9], [24], [23, 11, 10], [28], [14], [15, 13, 7]], [[18, 12], [15, 13, 8]]]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.int64)\n",
    "segment_a = bert_tokenizer.tokenize(examples[\"text_a\"])\n",
    "segment_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87385ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lookup_table = \"\"\"WordPiece_local/bert-large-uncased-vocab.txt\"\"\"\n",
    "# suffix_indicator='##', \n",
    "# max_bytes_per_word=100,\n",
    "# max_chars_per_token=None, \n",
    "# token_out_type=dtypes.int64,\n",
    "# unknown_token='[UNK]', \n",
    "# split_unknown_characters=False,\n",
    "# lower_case=False, \n",
    "# keep_whitespace=False, \n",
    "# normalization_form=None,\n",
    "# preserve_unused_token=False, \n",
    "# basic_tokenizer_class=BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ba0e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f6/t799ksr91mgfcn0jpk73scp00000gn/T/ipykernel_10229/1129673017.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dtypes'"
     ]
    }
   ],
   "source": [
    "import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3577f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = text.BertTokenizer(\n",
    "    vocab_lookup_table, suffix_indicator='##', max_bytes_per_word=100,\n",
    "    max_chars_per_token=None, \n",
    "    # token_out_type=dtypes.int64,\n",
    "    unknown_token='[UNK]', split_unknown_characters=False,\n",
    "    lower_case=False, keep_whitespace=False, normalization_form=None,\n",
    "    preserve_unused_token=False, \n",
    "    # basic_tokenizer_class=BasicTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973ba895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[7592], [2088]]]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tokenize(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef624ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[b'hello'], [b'world'], [b','], [b','], [b','], [b';'], [b'lkj'], [b';'], [b'lkj'], [b';'], [b'lkjiuginunnj'], [b'\\xef\\xbc\\x8c']]]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.detokenize(test.tokenize(\"hello world, , ,;lkj;lkj;lkjiuginunnj，  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b1665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
