{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf523cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b871fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: usa.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: foo\n",
      "  user_defined_symbols: bar\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: usa.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 1323 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: foo\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: bar\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=203764\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9524% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=82\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999524\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 1323 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 19873 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 1323\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 9873\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 9873 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=7942 obj=14.5105 num_tokens=22952 num_tokens/piece=2.88995\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=7136 obj=12.425 num_tokens=23109 num_tokens/piece=3.23837\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=5343 obj=12.5427 num_tokens=24706 num_tokens/piece=4.62399\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=5335 obj=12.4164 num_tokens=24720 num_tokens/piece=4.63355\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=4001 obj=12.8171 num_tokens=27098 num_tokens/piece=6.77281\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=4000 obj=12.6897 num_tokens=27104 num_tokens/piece=6.776\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3000 obj=13.2469 num_tokens=29998 num_tokens/piece=9.99933\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=3000 obj=13.1074 num_tokens=29998 num_tokens/piece=9.99933\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2250 obj=13.7489 num_tokens=33233 num_tokens/piece=14.7702\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2250 obj=13.6125 num_tokens=33233 num_tokens/piece=14.7702\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1687 obj=14.312 num_tokens=36662 num_tokens/piece=21.7321\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1687 obj=14.1813 num_tokens=36664 num_tokens/piece=21.7333\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1265 obj=14.9006 num_tokens=39942 num_tokens/piece=31.5747\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1265 obj=14.769 num_tokens=39947 num_tokens/piece=31.5787\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1100 obj=15.1026 num_tokens=41493 num_tokens/piece=37.7209\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1100 obj=15.0467 num_tokens=41493 num_tokens/piece=37.7209\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    # input='test/botchan.txt', \n",
    "    input='usa.txt', \n",
    "    model_prefix='m', \n",
    "    vocab_size=1000, \n",
    "    user_defined_symbols=['foo', 'bar']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cdf0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d9a2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.Load(\"m.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "233b1508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Th', 'is', '▁is', '▁a', '▁t', 'est']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsPieces(\"This is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f2d9c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[361, 99, 104, 42, 258, 165]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsIds(\"This is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f72f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c112569e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: China_zh.txt\n",
      "  input_format: \n",
      "  model_prefix: zh\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: foo\n",
      "  user_defined_symbols: bar\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: China_zh.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 1884 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: foo\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: bar\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=138513\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=2231\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 1868 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 22002 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 1868\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 5540\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 5540 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11753 obj=54.686 num_tokens=45205 num_tokens/piece=3.84625\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11010 obj=48.0874 num_tokens=45416 num_tokens/piece=4.12498\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8249 obj=49.7368 num_tokens=48366 num_tokens/piece=5.86326\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8240 obj=49.2002 num_tokens=48387 num_tokens/piece=5.87221\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=6180 obj=52.2237 num_tokens=52683 num_tokens/piece=8.52476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=6180 obj=51.5518 num_tokens=52710 num_tokens/piece=8.52913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=4634 obj=55.4386 num_tokens=57764 num_tokens/piece=12.4653\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=4634 obj=54.7008 num_tokens=57769 num_tokens/piece=12.4663\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3475 obj=59.8181 num_tokens=64238 num_tokens/piece=18.4858\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=3475 obj=58.9837 num_tokens=64238 num_tokens/piece=18.4858\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3300 obj=59.9347 num_tokens=65568 num_tokens/piece=19.8691\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=3300 obj=59.7838 num_tokens=65568 num_tokens/piece=19.8691\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: zh.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: zh.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    # input='test/botchan.txt', \n",
    "    input='China_zh.txt', \n",
    "    model_prefix='zh', \n",
    "    vocab_size=3000, \n",
    "    user_defined_symbols=['foo', 'bar']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83d76a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_zh = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96c455bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_zh.Load(\"zh.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16cb89e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " '成',\n",
       " '都',\n",
       " '市',\n",
       " '于',\n",
       " '2007',\n",
       " '年',\n",
       " '被',\n",
       " '中华人民共和国',\n",
       " '国家',\n",
       " '旅游',\n",
       " '局',\n",
       " '与',\n",
       " '世界',\n",
       " '旅游',\n",
       " '组织',\n",
       " '命',\n",
       " '名',\n",
       " '为',\n",
       " '“',\n",
       " '中国',\n",
       " '最',\n",
       " '佳',\n",
       " '旅游',\n",
       " '城市',\n",
       " '”。',\n",
       " '▁2021',\n",
       " '年',\n",
       " '夏',\n",
       " '季',\n",
       " '世界',\n",
       " '大学',\n",
       " '生',\n",
       " '运动',\n",
       " '会',\n",
       " '[15',\n",
       " ']',\n",
       " '和',\n",
       " '20',\n",
       " '25',\n",
       " '年',\n",
       " '世界',\n",
       " '运动',\n",
       " '会',\n",
       " '将',\n",
       " '在',\n",
       " '成',\n",
       " '都',\n",
       " '举',\n",
       " '办',\n",
       " '。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_zh.EncodeAsPieces(\n",
    "    \"\"\"\n",
    "    成都市于2007年被中华人民共和国国家旅游局与世界旅游组织命名为“中国最佳旅游城市”。\n",
    "    2021年夏季世界大学生运动会[15]和2025年世界运动会将在成都举办。\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89e65dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 279,\n",
       " 359,\n",
       " 123,\n",
       " 48,\n",
       " 1024,\n",
       " 8,\n",
       " 320,\n",
       " 22,\n",
       " 78,\n",
       " 978,\n",
       " 291,\n",
       " 66,\n",
       " 90,\n",
       " 978,\n",
       " 516,\n",
       " 930,\n",
       " 93,\n",
       " 38,\n",
       " 57,\n",
       " 42,\n",
       " 192,\n",
       " 1854,\n",
       " 978,\n",
       " 285,\n",
       " 964,\n",
       " 596,\n",
       " 8,\n",
       " 1354,\n",
       " 1194,\n",
       " 90,\n",
       " 365,\n",
       " 141,\n",
       " 636,\n",
       " 82,\n",
       " 1030,\n",
       " 16,\n",
       " 20,\n",
       " 108,\n",
       " 211,\n",
       " 8,\n",
       " 90,\n",
       " 636,\n",
       " 82,\n",
       " 172,\n",
       " 29,\n",
       " 279,\n",
       " 359,\n",
       " 2988,\n",
       " 816,\n",
       " 17]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_zh.EncodeAsIds(\n",
    "    \"\"\"\n",
    "    成都市于2007年被中华人民共和国国家旅游局与世界旅游组织命名为“中国最佳旅游城市”。\n",
    "    2021年夏季世界大学生运动会[15]和2025年世界运动会将在成都举办。\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3e48179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '维基', '百', '科', ',', '维基', '语', '录', ',', '维基', '新闻', ',', '中华人民共和国']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_zh.EncodeAsPieces(\n",
    "    \"\"\"\n",
    "    维基百科，维基语录，维基新闻,中华人民共和国\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce018df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
